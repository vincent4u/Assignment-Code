{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZB3b1kjzNluDTfwSpoF3M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d379b2685f542d0b8228ce539bf8505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7035f14ea859401a8a312028ceb920e5",
              "IPY_MODEL_4a49f172b7704e09b0631312fc621fbf",
              "IPY_MODEL_4eefa4fbdb344ec0ac10fae7b1211a56"
            ],
            "layout": "IPY_MODEL_92509229ffa044f4b46b17757b39731c"
          }
        },
        "7035f14ea859401a8a312028ceb920e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13764626888a4b0992b9401bb925e6f4",
            "placeholder": "​",
            "style": "IPY_MODEL_889a9edd28514e208ce66b8101506ebf",
            "value": "100%"
          }
        },
        "4a49f172b7704e09b0631312fc621fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68e506c95dd443e5a757b2fac4477cc7",
            "max": 8699,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ac9ab75a59f4c22b60abde035a19462",
            "value": 8699
          }
        },
        "4eefa4fbdb344ec0ac10fae7b1211a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e08fbee149234e9c90938adf1b7908aa",
            "placeholder": "​",
            "style": "IPY_MODEL_55ac16797c38445b8fa80203e591bf10",
            "value": " 8699/8699 [00:17&lt;00:00, 629.52it/s]"
          }
        },
        "92509229ffa044f4b46b17757b39731c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13764626888a4b0992b9401bb925e6f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "889a9edd28514e208ce66b8101506ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68e506c95dd443e5a757b2fac4477cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac9ab75a59f4c22b60abde035a19462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e08fbee149234e9c90938adf1b7908aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55ac16797c38445b8fa80203e591bf10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1656cc89e7a0425f861901a8608fc4de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8df407bc760d41a1830cc22f52203ba5",
              "IPY_MODEL_e330a43d0263455e8e1d3870bda11f6c",
              "IPY_MODEL_41a6cb6ed48f451c878a408624247ab8"
            ],
            "layout": "IPY_MODEL_e3aa0d82134741babaec1e43c43ebf8d"
          }
        },
        "8df407bc760d41a1830cc22f52203ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9a14a30e3c84c78a016847b1c457356",
            "placeholder": "​",
            "style": "IPY_MODEL_cb5291aeaee44de690ef2c418553edac",
            "value": "100%"
          }
        },
        "e330a43d0263455e8e1d3870bda11f6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a71eadbbc4c4312babbb791187a9cf9",
            "max": 2920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c68d9ebc7274e0497fa689e9e94ffba",
            "value": 2920
          }
        },
        "41a6cb6ed48f451c878a408624247ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e95f14f8a06c4c1b9d1fa290838af8d0",
            "placeholder": "​",
            "style": "IPY_MODEL_b87e6e6d93ae432f89cad8c6e4bf2984",
            "value": " 2920/2920 [00:05&lt;00:00, 638.51it/s]"
          }
        },
        "e3aa0d82134741babaec1e43c43ebf8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9a14a30e3c84c78a016847b1c457356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb5291aeaee44de690ef2c418553edac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a71eadbbc4c4312babbb791187a9cf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c68d9ebc7274e0497fa689e9e94ffba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e95f14f8a06c4c1b9d1fa290838af8d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b87e6e6d93ae432f89cad8c6e4bf2984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent4u/Assignment-Code/blob/master/look_into_this.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFcHMxZi4PFN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "# Load the text data\n",
        "X = [\"This is a hate speech tweet.\", \"I love this product.\", \"Minorities are not welcome here.\", \"The service was great!\"]\n",
        "y = [1, 0, 1, 0]  # 1 for hate speech, 0 for non-hate speech\n",
        "\n",
        "# Extract text features using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "# Perform cross-validation to evaluate the GMM model\n",
        "kf = KFold(n_splits=1, shuffle=True, random_state=42)\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X_vectorized):\n",
        "    X_train, X_test = X_vectorized[train_index], X_vectorized[test_index]\n",
        "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
        "\n",
        "    # Train the Gaussian Mixture Model\n",
        "    gmm = GaussianMixture(n_components=2, covariance_type='full')\n",
        "    gmm.fit(X_train.toarray())\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = gmm.predict(X_test.toarray())\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    accuracies.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Print the average performance metrics\n",
        "print(f\"Accuracy: {np.mean(accuracies):.2f}\")\n",
        "print(f\"Precision: {np.mean(precisions):.2f}\")\n",
        "print(f\"Recall: {np.mean(recalls):.2f}\")\n",
        "print(f\"F1-score: {np.mean(f1_scores):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCkEVsz0_GAW",
        "outputId": "58ebfdb1-bf81-444d-d58f-090bc3f9809b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "import lightgbm as lgb\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove specific tokens\n",
        "    text = text.replace(\"SDATA_5 :\", \"\").replace(\"EDATA_5\", \"\").replace(\"NEWLINE_TOKEN\", \"\")\n",
        "\n",
        "\n",
        "    # Remove special characters and punctuation marks using regular expressions\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove URLs, email addresses, and phone numbers\n",
        "    text = re.sub(r\"http\\S+|www\\S+|\\S+@\\S+|\\d{10}\", \"\", text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "    # Remove non-ASCII characters\n",
        "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
        "\n",
        "    # Convert the text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra white spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Join the tokens back into a single string\n",
        "    preprocessed_text = \" \".join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Preprocess the text\n",
        "train_df['preprocessed_comment'] = train_df['comment'].apply(preprocess_text)\n",
        "val_df['preprocessed_comment'] = val_df['comment'].apply(preprocess_text)\n",
        "\n",
        "print(train_df['preprocessed_comment'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_xgRlvV-zcy",
        "outputId": "05b4cd16-e5ce-4eda-d025-cf2252944739"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       kindergarden ignorantsve added pakistani artic...\n",
            "1                   violate rrv rule may banned wikipedia\n",
            "2                     judging message tp could use advice\n",
            "3                                     love fatty sex mmmm\n",
            "4        please could explain thinking warning page thank\n",
            "                              ...                        \n",
            "8694    utci beg differ history hugely important licen...\n",
            "8695           penis sex grawp haeppenispenisvaginavagina\n",
            "8696                      thats exactly gamer dying breed\n",
            "8697                          article unless fact support\n",
            "8698                milton friedman lp also measure thing\n",
            "Name: preprocessed_comment, Length: 8699, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaMulticore\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "train_df['preprocessed_comment'] = train_df['comment'].apply(preprocess_text)\n",
        "val_df['preprocessed_comment'] = val_df['comment'].apply(preprocess_text)\n",
        "\n",
        "X_train = train_df['preprocessed_comment'].tolist()\n",
        "y_train = train_df['toxicity'].tolist()\n",
        "\n",
        "X_val = val_df['preprocessed_comment'] .tolist()\n",
        "y_val = val_df['toxicity'].tolist()\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = lambda text: text.split()\n",
        "X_train_tokens = [tokenizer(text) for text in X_train]\n",
        "X_val_tokens = [tokenizer(text) for text in X_val]\n",
        "\n",
        "# Create the bag-of-words representation\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_val_bow = vectorizer.transform(X_val)\n",
        "\n",
        "# Create the LDA model\n",
        "dictionary = corpora.Dictionary(X_train_tokens)\n",
        "corpus = [dictionary.doc2bow(text) for text in X_train_tokens]\n",
        "lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=2, passes=10, iterations=500)\n",
        "\n",
        "# Get the topic distributions for the training and validation data\n",
        "X_train_topics = [lda_model[doc] for doc in corpus]\n",
        "X_val_topics = [lda_model[dictionary.doc2bow(text)] for text in X_val_tokens]\n",
        "\n",
        "# Pad the topic distributions to a fixed length\n",
        "max_topics = max(len(doc_topics) for doc_topics in X_train_topics)\n",
        "X_train_topics_padded = [doc_topics + [0] * (max_topics - len(doc_topics)) for doc_topics in X_train_topics]\n",
        "X_val_topics_padded = [doc_topics + [0] * (max_topics - len(doc_topics)) for doc_topics in X_val_topics]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train_topics_padded = np.array(X_train_topics_padded)\n",
        "X_val_topics_padded = np.array(X_val_topics_padded)\n",
        "\n",
        "# Train a classifier on the padded topic distributions\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train_topics_padded, y_train)\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "y_pred = clf.predict(X_val_topics_padded)\n",
        "\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "precision = precision_score(y_val, y_pred)\n",
        "recall = recall_score(y_val, y_pred)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "EmTX41-i6gPX",
        "outputId": "09dfcb0f-fc59-4e3b-8a15-cf8e87987b49"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (8699, 2) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e17e3d199cb8>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Convert to numpy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mX_train_topics_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_topics_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mX_val_topics_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_topics_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (8699, 2) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the data\n",
        "train_df['preprocessed_comment'] = train_df['comment'].apply(preprocess_text)\n",
        "val_df['preprocessed_comment'] = val_df['comment'].apply(preprocess_text)\n",
        "\n",
        "X_train = train_df['preprocessed_comment'].tolist()\n",
        "y_train = train_df['toxicity'].tolist()\n",
        "\n",
        "X_val = val_df['preprocessed_comment'] .tolist()\n",
        "y_val = val_df['toxicity'].tolist()\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = lambda text: text.split()\n",
        "X_train_tokens = [tokenizer(text) for text in X_train]\n",
        "X_val_tokens = [tokenizer(text) for text in X_val]\n",
        "\n",
        "# Create the bag-of-words representation\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_val_bow = vectorizer.transform(X_val)\n",
        "\n",
        "# Train the Naive Bayes model\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_bow, y_train)\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "y_pred = clf.predict(X_val_bow)\n",
        "\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "precision = precision_score(y_val, y_pred)\n",
        "recall = recall_score(y_val, y_pred)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "uO2Ac0UP-QFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "val_df = pd.read_csv('/content/valid.csv')\n",
        "\n",
        "# Preprocess the text (assuming you have a preprocess_text function)\n",
        "train_df['preprocessed_comment'] = train_df['comment'].apply(preprocess_text)\n",
        "val_df['preprocessed_comment'] = val_df['comment'].apply(preprocess_text)\n",
        "\n",
        "X_train = train_df['preprocessed_comment'].tolist()\n",
        "y_train = train_df['toxicity'].tolist()\n",
        "\n",
        "X_val = val_df['preprocessed_comment'].tolist()\n",
        "y_val = val_df['toxicity'].tolist()\n",
        "\n",
        "# Create the bag-of-words representation\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_val_bow = vectorizer.transform(X_val)\n",
        "\n",
        "# Train the Naive Bayes model\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_bow, y_train)\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "y_pred = clf.predict(X_val_bow)\n",
        "\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "precision = precision_score(y_val, y_pred)\n",
        "recall = recall_score(y_val, y_pred)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6LwnEdeA24M",
        "outputId": "e73adf74-ffdd-4f4c-f950-9792ec1139b1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.87\n",
            "Precision: 0.21\n",
            "Recall: 0.01\n",
            "F1-score: 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "val_df = pd.read_csv('/content/valid.csv')\n",
        "\n",
        "# Preprocess the text (assuming you have a preprocess_text function)\n",
        "train_df['preprocessed_comment'] = train_df['comment'].apply(preprocess_text)\n",
        "val_df['preprocessed_comment'] = val_df['comment'].apply(preprocess_text)\n",
        "\n",
        "X_train = train_df['preprocessed_comment'].tolist()\n",
        "y_train = train_df['toxicity'].tolist()\n",
        "\n",
        "X_val = val_df['preprocessed_comment'].tolist()\n",
        "y_val = val_df['toxicity'].tolist()\n",
        "\n",
        "# Create the bag-of-words representation\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_val_bow = vectorizer.transform(X_val)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0]\n",
        "}\n",
        "\n",
        "# Create the Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1')\n",
        "grid_search.fit(X_train_bow, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the validation data\n",
        "y_pred = best_model.predict(X_val_bow)\n",
        "\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "precision = precision_score(y_val, y_pred)\n",
        "recall = recall_score(y_val, y_pred)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "tK2VTdPvBKPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiyqItgBI0ti",
        "outputId": "df5a1e7d-f440-4df6-ae1a-7a90bdcfb148"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hybrid model\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.base import TransformerMixin\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "val_df = pd.read_csv('/content/valid.csv')\n",
        "\n",
        "# Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Implement your text preprocessing function here\n",
        "    text = text.replace(\"SDATA_5 :\", \"\").replace(\"EDATA_5\", \"\").replace(\"NEWLINE_TOKEN\", \"\")\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters and punctuation\n",
        "    text = re.sub(r\"http\\S+|www\\S+|\\S+@\\S+|\\d{10}\", \"\", text)  # Remove URLs, email addresses, and phone numbers\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
        "    text = text.encode(\"ascii\", \"ignore\").decode()  # Remove non-ASCII characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra white spaces\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words and apply stemming\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Join the tokens back into a single string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "train_df['preprocessed_comment'] = train_df['comment'].apply(preprocess_text)\n",
        "val_df['preprocessed_comment'] = val_df['comment'].apply(preprocess_text)\n",
        "\n",
        "# Feature extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_df['preprocessed_comment'])\n",
        "X_val = vectorizer.transform(val_df['preprocessed_comment'])\n",
        "y_train = train_df['toxicity']\n",
        "y_val = val_df['toxicity']\n",
        "\n",
        "# Fit the LDA model on the entire training data\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "X_train_lda = lda.fit_transform(X_train)\n",
        "X_val_lda = lda.transform(X_val)\n",
        "\n",
        "# Naive Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# XGBoost Classifier\n",
        "xgb = XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "xgb.fit(X_train_lda, y_train)\n",
        "\n",
        "# Create a custom transformer to apply the Naive Bayes model\n",
        "class NBTransformer(TransformerMixin):\n",
        "    def __init__(self, nb_model):\n",
        "        self.nb_model = nb_model\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.nb_model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.nb_model.predict_proba(X)\n",
        "\n",
        "# Create the hybrid model\n",
        "hybrid_model = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('lda', lda),\n",
        "        ('nb', NBTransformer(nb))\n",
        "    ])),\n",
        "    ('xgb', xgb)\n",
        "])\n",
        "\n",
        "# Fit the hybrid model\n",
        "hybrid_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the hybrid model\n",
        "y_pred = hybrid_model.predict(X_val)\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "precision = precision_score(y_val, y_pred)\n",
        "recall = recall_score(y_val, y_pred)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdUVroltF_-Q",
        "outputId": "b1ce59e6-3522-490a-c0af-59e0eef1f8e0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.85\n",
            "Precision: 0.19\n",
            "Recall: 0.04\n",
            "F1-score: 0.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LDA model alone\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm.auto import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "val_df = pd.read_csv('/content/valid.csv')\n",
        "\n",
        "# Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters, URLs, and numbers using regex\n",
        "    text = re.sub(r'[^\\w\\s]|http\\S+|www\\S+|\\S+@\\S+|\\d+', '', text)\n",
        "\n",
        "    # Convert to lowercase and remove extra whitespace\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocess the text in a single pass\n",
        "train_df['preprocessed_comment'] = train_df['comment'].apply(preprocess_text)\n",
        "val_df['preprocessed_comment'] = val_df['comment'].apply(preprocess_text)\n",
        "\n",
        "# Create the CountVectorizer and LDA pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('lda', LatentDirichletAllocation(n_components=10, random_state=42))\n",
        "])\n",
        "\n",
        "# Fit and transform the data in parallel\n",
        "X_train = pipeline.fit_transform(train_df['preprocessed_comment'])\n",
        "X_val = pipeline.transform(val_df['preprocessed_comment'])\n",
        "\n",
        "y_train = train_df['toxicity']\n",
        "y_val = val_df['toxicity']\n",
        "\n",
        "# Evaluate the model in parallel\n",
        "def evaluate_model(y_val, y_pred_labels):\n",
        "    accuracy = accuracy_score(y_val, y_pred_labels)\n",
        "    precision = precision_score(y_val, y_pred_labels, average='macro', zero_division='warn')\n",
        "    recall = recall_score(y_val, y_pred_labels, average='macro', zero_division='warn')\n",
        "    f1 = f1_score(y_val, y_pred_labels, average='macro')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "y_pred = pipeline.transform(val_df['preprocessed_comment'])\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "accuracy, precision, recall, f1 = evaluate_model(y_val, y_pred_labels)\n",
        "\n",
        "print(f\"accuracy = {accuracy:.2f}\")\n",
        "print(f\"precision = {precision:.2f}\")\n",
        "print(f\"recall = {recall:.2f}\")\n",
        "print(f\"f1 = {f1:.2f}\")"
      ],
      "metadata": {
        "id": "Rb2f7iwNLG63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Naive Bayes  only\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tqdm.auto import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "val_df = pd.read_csv('/content/valid.csv')\n",
        "\n",
        "# Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters, URLs, and numbers using regex\n",
        "    text = re.sub(r'[^\\w\\s]|http\\S+|www\\S+|\\S+@\\S+|\\d+', '', text)\n",
        "\n",
        "    # Convert to lowercase and remove extra whitespace\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocess the text in a single pass\n",
        "tqdm.pandas()  # Enable progress_apply for pandas\n",
        "train_df['preprocessed_comment'] = train_df['comment'].progress_apply(preprocess_text)\n",
        "val_df['preprocessed_comment'] = val_df['comment'].progress_apply(preprocess_text)\n",
        "\n",
        "# Create the Naive Bayes pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline\n",
        "pipeline.fit(train_df['preprocessed_comment'], train_df['toxicity'])\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = pipeline.predict(val_df['preprocessed_comment'])\n",
        "\n",
        "accuracy = accuracy_score(val_df['toxicity'], y_pred)\n",
        "precision = precision_score(val_df['toxicity'], y_pred, average='macro', zero_division='warn')\n",
        "recall = recall_score(val_df['toxicity'], y_pred, average='macro', zero_division='warn')\n",
        "f1 = f1_score(val_df['toxicity'], y_pred, average='macro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "t_23kSJTTdrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LDA and Logistic regression\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "val_df = pd.read_csv('/content/valid.csv')\n",
        "\n",
        "# Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters, URLs, and numbers using regex\n",
        "    text = re.sub(r'[^\\w\\s]|http\\S+|www\\S+|\\S+@\\S+|\\d+', '', text)\n",
        "\n",
        "    # Convert to lowercase and remove extra whitespace\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocess the text in a single pass\n",
        "tqdm.pandas()  # Enable progress_apply for pandas\n",
        "train_df['preprocessed_comment'] = train_df['comment'].progress_apply(preprocess_text)\n",
        "val_df['preprocessed_comment'] = val_df['comment'].progress_apply(preprocess_text)\n",
        "\n",
        "# Create the LDA pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('lda', LatentDirichletAllocation(n_components=10, random_state=42))  # Adjust n_components as needed\n",
        "])\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train = pipeline.fit_transform(train_df['preprocessed_comment'])\n",
        "X_val = pipeline.transform(val_df['preprocessed_comment'])\n",
        "\n",
        "# Your training and validation labels\n",
        "y_train = train_df['toxicity']\n",
        "y_val = val_df['toxicity']\n",
        "\n",
        "# Train Logistic Regression classifier\n",
        "classifier = LogisticRegression(random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_pred = classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "precision = precision_score(y_val, y_pred, average='macro', zero_division='warn')\n",
        "recall = recall_score(y_val, y_pred, average='macro', zero_division='warn')\n",
        "f1 = f1_score(y_val, y_pred, average='macro')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "0d379b2685f542d0b8228ce539bf8505",
            "7035f14ea859401a8a312028ceb920e5",
            "4a49f172b7704e09b0631312fc621fbf",
            "4eefa4fbdb344ec0ac10fae7b1211a56",
            "92509229ffa044f4b46b17757b39731c",
            "13764626888a4b0992b9401bb925e6f4",
            "889a9edd28514e208ce66b8101506ebf",
            "68e506c95dd443e5a757b2fac4477cc7",
            "9ac9ab75a59f4c22b60abde035a19462",
            "e08fbee149234e9c90938adf1b7908aa",
            "55ac16797c38445b8fa80203e591bf10",
            "1656cc89e7a0425f861901a8608fc4de",
            "8df407bc760d41a1830cc22f52203ba5",
            "e330a43d0263455e8e1d3870bda11f6c",
            "41a6cb6ed48f451c878a408624247ab8",
            "e3aa0d82134741babaec1e43c43ebf8d",
            "c9a14a30e3c84c78a016847b1c457356",
            "cb5291aeaee44de690ef2c418553edac",
            "4a71eadbbc4c4312babbb791187a9cf9",
            "0c68d9ebc7274e0497fa689e9e94ffba",
            "e95f14f8a06c4c1b9d1fa290838af8d0",
            "b87e6e6d93ae432f89cad8c6e4bf2984"
          ]
        },
        "id": "WjV50dtoVpBy",
        "outputId": "4f5facd8-715e-4786-8221-e8df93448016"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/8699 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d379b2685f542d0b8228ce539bf8505"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2920 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1656cc89e7a0425f861901a8608fc4de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.87\n",
            "Precision: 0.43\n",
            "Recall: 0.50\n",
            "F1 Score: 0.46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}